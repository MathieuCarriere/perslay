{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tutorial for \n",
    "\n",
    "# *PersLay: A Simple and Versatile Neural Network Layer for Persistence Diagrams*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the current version of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version of your system: (we recommand Python 3.6)\n",
      "3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Current version of your system: (we recommand Python 3.6)\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.utils import generate, visualization, single_run\n",
    "from archi import perslay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "In this notebook:\n",
    "- First, we select a dataset. Two types of datasets are provided by default, either synthetic orbits from dynamical systems, or real-life graph dataset. \n",
    "- Then, we generate the persistence diagrams (and other useful informations such as labels, etc.) for the chosen dataset.\n",
    "- (Optional) we propose to visualize the generated diagrams.\n",
    "- We define a neural network that uses some PersLay channels as first layers to handle persistence diagrams. This can be used as a guideline to use PersLay in your own experiments.\n",
    "- We show how to train this neural network on the chosen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by choosing the dataset we want to run the experiments on.\n",
    "\n",
    "We suggest the user to start with `\"MUTAG\"` as this dataset is reasonably small (188 graphs with 18 nodes on average). Note that its small size implies a large variability in tests.\n",
    "\n",
    "Available options are:\n",
    "\n",
    "- Orbit datasets: `\"ORBIT5K\"`, `\"ORBIT100K\"`.\n",
    "\n",
    "- Graphs datasets: `\"MUTAG\"`, `\"BZR\"`, `\"COX2\"`, `\"DHFR\"`, `\"PROTEINS\"`, `\"NCI1\"`, `\"NCI109\"`, `\"FRANKENSTEIN\"`,  `\"IMDB-BINARY\"`, `\"IMDB-MULTI\"`.\n",
    "\n",
    "__Important note:__ `COLLAB`, `\"REDDIT5K\"` and `\"REDDIT12K\"` are not available yet (see README.md). Contact the authors for more information.\n",
    "\n",
    "Beware that for the larger datasets (`COLLAB`, `REDDIT5K, REDDIT12K, ORBIT100K`), the needed files can be quite large (e.g. 3Gb for for `ORBIT100K`), so that RAM can be limiting, and time to generate the diagrams and running the experiments can be quite involving depending on the hardware available. You can have access to a description of the dataset in the Section B in the supplementary material of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose your config file using one of the filename mentioned above.\n",
    "dataset = \"MUTAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building persistence diagrams and eventual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implicitely load our data (saved as `.mat` files for graphs datasets, and generated on-the-fly for orbits datasets---which can take some time for `ORBIT100K` especially), and then compute the persistence diagrams that will be used in the classification experiment (requires to have `gudhi` installed). For graph datasets, we also generate a series of additional features (see [1]).\n",
    "\n",
    "Running `generate` will store diagrams, features and labels. Therefore, it is sufficient to run it just once (for each different dataset).\n",
    "\n",
    "Note that for bigger datasets, the computations of these diagrams can be quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualise some example of diagrams generated.\n",
    "# Requires matplotlib.\n",
    "visualization(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PersLay in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a PersLay layer, and then a (very simple) neural network architecture that uses PersLay. This can be used as a template to build your own architecture using PersLay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer type, must be one of (see [1] for details):\n",
    "- `\"im\"` for persistence image layer.\n",
    "- `\"pm\"` for permutation equivarient layer (as in [2]).\n",
    "- `\"gs\"` for a gaussian layer.\n",
    "- `\"ls\"` for a landscape layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = \"im\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation invariant operator, must be one of:\n",
    "- `\"sum\"`.\n",
    "- `\"topk\"`, will select the $k$ highest values, specified in `keep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_op = \"sum\"\n",
    "keep = 5  # only useful if perm_op = \"topk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight, must be one of\n",
    "- `\"grid\"`, if so, one must pick a grid_size.\n",
    "- `\"linear\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=\"grid\"\n",
    "grid_size = [10, 10]  # Only used if weight==\"grid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are some hyper parameters that are specific to different the layer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter specific to layer_type=\"im\"\n",
    "image_size=[10, 10]\n",
    "# Parameter specific to layer_type=\"gs\"\n",
    "num_gaussians=50\n",
    "# Parameter specific to layer_type=\"pm\"\n",
    "d = 50  # Output dimension\n",
    "# Parameter specific to layer_type=\"ls\"\n",
    "num_sample = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ There are some other parameters available to tune PersLay that are not detailed here. This will be updated later. Feel free to check the implementation provided in `archi.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the template below, we define a very simple `model` that encodes a network architecture. In this model, we define a PersLay layer for each type of diagrams used in input, but all these layers have the same hyper-parameters (as in [1]).\n",
    "\n",
    "Eventual additional features are simply concatenated with the output of these perslay layers, and a fully connected layer is then used to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(feats, diags, parameters, num_filts, num_labels):\n",
    "    list_v = []\n",
    "    for i in range(num_filts):\n",
    "        # A perslay channel must be defined for each type of diagram\n",
    "        # Here, they all have the same hyper-parameters.\n",
    "        perslay(output=list_v, # the vector use to store the output of all perslay\n",
    "                name=\"perslay-\" + str(i),  # name of this layer\n",
    "                diag=diags[i],  # this layer handle the i-th type of diagrams\n",
    "                layer=layer_type, \n",
    "                perm_op=perm_op,\n",
    "                keep=keep,\n",
    "                persistence_weight=weight,\n",
    "                grid_size=grid_size,\n",
    "                image_size=image_size,\n",
    "                num_gaussians=num_gaussians,\n",
    "                num_samples=num_samples,\n",
    "                peq=[(d,None)]\n",
    "                )\n",
    "\n",
    "        # Concatenate all channels and add other features\n",
    "        vector = tf.concat(list_v, 1)\n",
    "    with tf.variable_scope(\"norm_feat\"):\n",
    "        feat = tf.layers.batch_normalization(feats)\n",
    "\n",
    "    vector = tf.concat([vector, feat], 1)\n",
    "\n",
    "    #  Final layer to make predictions\n",
    "    with tf.variable_scope(\"final-dense-3\"):\n",
    "        vector = _post_processing(tf.layers.dense(vector, num_labels), \"\")\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our network, following the architecture illustrated in the article (Figure 3)<WARNING CHECK FIG LABEL>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any neural-network framework, PersLay benefits from the use of GPU(s). If a GPU is available (and `tensorflow-gpu` is installed), the computations should hopefully use it. Otherwise, the computations will be run on the cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest the user to run a single-run first with the `single_run` function, that is training the network once and observing the performance (classification accuracy) on the test set.\n",
    "- For orbit datasets, the train-test split is 70-30 (to be consistent with [LY18]).\n",
    "- For graph datasets, the train-test split is 90-10 (to be consistent with [ZWX+18]).\n",
    "\n",
    "The `single_run` function will load (and print) the network parameters as described in Table 5<CHECK LABEL>: perslay hyperparameters (choice of $\\phi$, $w$...), optimizer (number of epochs, learning rate...), etc.\n",
    "   \n",
    "It then uses the diagrams (and eventual features) that have been generated when calling `generate(dataset)`, randomly split them into train/test sets, and use them to feed to the network. \n",
    "\n",
    "Train and Test accuracies are printed every 10 epochs during the training.\n",
    "\n",
    "Note that (especially on small datasets like `MUTAG, COX2` etc.), there can be an important variability in the accuracy reported on different calls of `single_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] _PersLay: A Simple and Versatile Neural Network Layer for Persistence Diagrams._\n",
    "Mathieu Carrière, Frederic Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, Yuhei Umeda.\n",
    "\n",
    "[2] _Deep Sets._\n",
    "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R. Salakhutdinov, Alexander J. Smola.\n",
    "_Advances in Neural Information Processing Systems 30 (NIPS 2017)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
